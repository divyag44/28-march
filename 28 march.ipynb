{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "01e2bf81-e465-460f-811d-8992a18160ea",
   "metadata": {},
   "source": [
    "## Q1. What is Ridge Regression, and how does it differ from ordinary least squares regression?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30342a64-7d0a-43bc-a615-44944ae7624c",
   "metadata": {},
   "source": [
    "### ### Ridge Regression\n",
    "\n",
    "### **Definition**: Ridge Regression is a linear regression technique that includes a regularization term to penalize large coefficients, helping to prevent overfitting.\n",
    "\n",
    "**Key Features**:\n",
    "- **Regularization Term**: Adds \\(\\lambda \\sum_{j=1}^{p} \\beta_j^2\\) to the cost function.\n",
    "- **Cost Function**: \n",
    "  \\[\n",
    "  J(\\beta) = \\sum_{i=1}^{n} (y_i - \\hat{y_i})^2 + \\lambda \\sum_{j=1}^{p} \\beta_j^2\n",
    "  \\]\n",
    "- **Shrinkage**: Coefficients are shrunk towards zero.\n",
    "\n",
    "### Differences from Ordinary Least Squares (OLS) Regression\n",
    "\n",
    "1. **Objective**:\n",
    "   - **OLS**: Minimizes \\(\\sum_{i=1}^{n} (y_i - \\hat{y_i})^2\\).\n",
    "   - **Ridge**: Minimizes \\(\\sum_{i=1}^{n} (y_i - \\hat{y_i})^2 + \\lambda \\sum_{j=1}^{p} \\beta_j^2\\).\n",
    "\n",
    "2. **Handling Multicollinearity**:\n",
    "   - **OLS**: Sensitive to multicollinearity.\n",
    "   - **Ridge**: Mitigates multicollinearity.\n",
    "\n",
    "3. **Overfitting**:\n",
    "   - **OLS**: Prone to overfitting.\n",
    "   - **Ridge**: Reduces overfitting.\n",
    "\n",
    "4. **Bias-Variance Tradeoff**:\n",
    "   - **OLS**: Low bias, high variance.\n",
    "   - **Ridge**: Slightly higher bias, lower variance.\n",
    "\n",
    "5. **Coefficient Estimates**:\n",
    "   - **OLS**: Can be large if predictors are highly correlated.\n",
    "   - **Ridge**: Coefficients are shrunk to reduce their size."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d14c1a9c-4f02-4494-b9d5-fdc2d381cf65",
   "metadata": {},
   "source": [
    "## Q2. What are the assumptions of Ridge Regression?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d60b849-b8df-4dc0-b121-b6b5f895c016",
   "metadata": {},
   "source": [
    "### The assumptions of Ridge Regression are similar to those of ordinary least squares (OLS) regression, with additional considerations due to the regularization aspect. Here are the key assumptions:\n",
    "\n",
    "Linearity: The relationship between the independent variables (predictors) and the dependent variable (response) is linear.\n",
    "\n",
    "Independence: The residuals (errors) are independent of each other. This means there is no correlation between consecutive residuals in time series data.\n",
    "\n",
    "Homoscedasticity: The residuals have constant variance at every level of the independent variables. This means that the spread or “scatter” of the residuals should be roughly the same across all levels of the predictors.\n",
    "\n",
    "Normality: The residuals are normally distributed, particularly important for constructing confidence intervals and hypothesis tests."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3bf0072b-007c-416c-9496-b5c4da418d97",
   "metadata": {},
   "source": [
    "## Q3. How do you select the value of the tuning parameter (lambda) in Ridge Regression?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0fc5e7a-687b-4ccb-ae74-e9627ca8f6f7",
   "metadata": {},
   "source": [
    "### ### Selecting the Tuning Parameter (\\(\\lambda\\)) in Ridge Regression\n",
    "\n",
    "1. **Cross-Validation**:\n",
    "   - **k-Fold Cross-Validation**: Split data into \\(k\\) folds, train on \\(k-1\\) folds, validate on the remaining fold, repeat \\(k\\) times, and choose \\(\\lambda\\) that minimizes average validation error.\n",
    "   - **Leave-One-Out Cross-Validation (LOOCV)**: Use one observation for validation and the rest for training, repeat for all observations, and select the optimal \\(\\lambda\\).\n",
    "\n",
    "2. **Grid Search**:\n",
    "   - Define a range of \\(\\lambda\\) values, perform cross-validation for each \\(\\lambda\\), and select the one with the best performance.\n",
    "\n",
    "3. **Regularization Path Algorithms**:\n",
    "   - Use algorithms like LARS to trace the path of coefficient estimates as \\(\\lambda\\) changes and select the best \\(\\lambda\\)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bad43b0c-cac2-47f3-8845-49140d567c6c",
   "metadata": {},
   "source": [
    "## Q4. Can Ridge Regression be used for feature selection? If yes, how?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c4b5bf8-20d9-42d0-8e48-0748677f3731",
   "metadata": {},
   "source": [
    "### Ridge Regression is not typically used for feature selection because it shrinks coefficients towards zero but does not set them exactly to zero. However, it can help in identifying important features by reducing the impact of less important ones.\n",
    "\n",
    "For explicit feature selection, **Lasso Regression** is preferred as it can shrink some coefficients exactly to zero, effectively selecting a subset of features.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c085524-1633-414b-b36d-227887665852",
   "metadata": {},
   "source": [
    "## Q5. How does the Ridge Regression model perform in the presence of multicollinearity?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adce0250-5c63-4dd9-83a2-a8d2d0059b39",
   "metadata": {},
   "source": [
    "## In the presence of multicollinearity, Ridge Regression performs well by adding a penalty to the size of the coefficients, which reduces their variance. This regularization helps stabilize the estimates and improve the model's generalization, making it more robust compared to Ordinary Least Squares (OLS) regression."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c04ffcc8-e1f6-4f5f-9c8c-57d02fdbb579",
   "metadata": {},
   "source": [
    "## Q6. Can Ridge Regression handle both categorical and continuous independent variables?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "223147e3-dfdf-4dfe-9c77-502ecfb5b0fc",
   "metadata": {},
   "source": [
    "## Yes, Ridge Regression can handle both categorical and continuous independent variables. However, categorical variables need to be encoded (e.g., one-hot encoding) before being used in the model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce7ec607-28f2-47cb-ad7c-7222258fdf18",
   "metadata": {},
   "source": [
    "### Q7. How do you interpret the coefficients of Ridge Regression?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6adabc3c-bdae-4f22-9b5f-ae9918dba767",
   "metadata": {},
   "outputs": [],
   "source": [
    "## "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
